TODO:
1.线程池中发现queueSize不是0时(发现acitveCount快达到了最大线程数时,也可报警),要监控报出错来
2.怎么样防止一个cmd太慢,而提早不响应,这样不影响其他业务

3.加了defalter后内存涨得太快?是否比较耗cpu?
4.是否要增加要增加对 编码解码超时的处理
5.如何对netty内部状态进行监控
6.是否要增加 worker的个数


须关注的官方问题：
1.Invalid version format when reading HTTP response
https://jira.jboss.org/browse/NETTY-369

2.ipFilter官方已经在准备实现：
https://issues.jboss.org/browse/NETTY-364

3.HttpServerConfig.releaseExternalResources()

4.http Post decoder
http://fisheye.jboss.org/browse/Netty/branches/httpPost/src/main/java/org/jboss/netty/handler/codec/http/HttpPostRequestDecoder.java?r2=2385&r1=1945


//TODO:是否可以实时设置cmdmapper.properites中的所有配置
//TODO:log access日志上是否打印出attach
//TODO:把mxBean的所有信息用 Json打印出来
//TODO:通过shellCmd异步实现tail -n 200 -f logFilexxx的功能
//TODO:如果开放grep后面的权限的话,怎么保证 防注入问题


2011-03-19
1.整合优化超时处理器(IdleStateHandler 研究)

SimpleIdleChannelHandler处理没有关闭的channel比较原始,其跟 业务线程中断器 并不是特别地配合地好
为什么一开始不用netty自带的 IdleStateHandler而重新写了个 SimpleIdleChannelHandler
其原因是 发现 IdleStateHandler是通过在pipeline中对应每一个channel都分配一个 idleStateHandler,内部再根据
readerIdleTimeSeconds, writerIdleTimeSeconds, allIdleTimeSeconds
这三个参数,初始化0到3个timer,然后在channelClose时再cancel掉这三个timer
作者为什么这样实现是基于其强大的Pipeline处理链来考虑的,思路很清晰简单
但个人认为这样生成的对象太多,而这三个timer的生命周期非常之短
为了实现这个IoIdle时关闭空闲channel的功能,这样的实现代价太大

为了跟业务线程中断器一起配合,现在重构XLContextAttachment

在具体的业务disaptcher中
channelOpen时,新建attach 这里直接注册一样生命周期的context
[channelBound mark lastIoTime］
[channelConn时,说明其准备 decode mark lastIoTime]

messageReceived时,说明其decode完毕,mark lastIoTime
(这里能通过httpReq来找到其decode的开始时间,其resp的createTime就是跟当前lastIoTime一致),
并注册最新的httpReq,httpResp(可以注册最新的messageEvent)

disaptch时,注册其业务线程,说明此线程可能会有长时间业务操作
业务处理完成时,注销其业务线程
真正wirteResponse时,是其开始encode的信号,因此mark lastIoTime(写)

writeComplete时,说明其已经encode完成,mark lastIoTime(写)

channelClose时,直接注销此attach,这是其生命周期终点

2011-03-18
1.不同cmd设置的超时时间,可以实时reload
2.在根本没有命令设置了超时的情况下,关闭后台线程 TimeoutInterrupter
3.OperMemCmd跟statMemCmd移动到 httptool包中

2011-03-17
1.不同cmd可以设置不同的超时时间
2.原来的业务中的关系是：
resp <-> ctx -> attach -> resp
                       -> req

ctx -> channel
    -> pipeline
现在整理成attach来当成桥梁：

ctx -> attach -> resp
               -> req
               -> cmdMeta
               -> messageEvent
               -> ctx

3.整理优化cmdMappers类
4.settingCmd.cmds中显示不同cmd的不同设置(通过title来弹出)
5.去掉不再维护的cmdPageDispatcher
6.response增加设置keepAliveTimeout的方法,以适应业务中不同情况下可以真正提前设置keepAliveTimeout
7.server在读client端的输入流时,如果client端关闭连接,server端(有时会)报connection time out,在BasePageDispatcher中不再对此情况打印堆栈
8.attach的toString中包含最简最必要信息,以供追踪流程
9.HttpUtil类移动到ServerUtil,兼容考虑暂时不删除


2011-03-15
1.XLHttpRequestDecoder中发现解析请求头失败时打印更详细的信息,以定位httpClient是否使用了不规范的http请求
2.增加shellCmd,实现页面输出常用命令

2011-03-11
1.XLHttpRequest的getRemoteIP()接口跟IPGetter引擎对接,可以通过配置forwardProxyHeaderNames=X-Real-IP来处理nginx前置时获取不了真实ip的问题

2011-02-16
1.增加接口/robots.txt,使整个服务器接口都不被爬虫抓取
2.完善ipfilter通过邮箱激活来实现临时提升ip权限的功能
3.默认关闭httpServer中,不处理内部boss线程及worker线程(因为调用erverBootstrap.releaseExternalResources()方法经常很慢),在一般情况下
如果服务器没有非常重要的安全数据需要回写时,可以直接System.exit(0)


2011-01-21
1.增加内部tps统计功能,/stat/tps 可以访问到最近的tps统计(文字版)
2.绕过netty自带CookieDecoder的bug问题
3.各个命令增加访问计数
4.TODO:各个命令可以单独配置toleranceTimeout(thread 的 中断。。并不是每时有效果)
5.TODO:各个命令有更详细的访问用时统计

2010-11-29
1.优化HttpServerPipelineFactory的getPipeline方法,让可配置的channelHandle在配置变动时就预先生成好
2.Bootstrap增加shutdownRunnalbe,可以在程序关闭前做特定操作
3.较早有增加：antiDos模块,xlhttpRequest增加cookie getter方法,启动时打印启动用时

2010-07-16
1.增加@Config动态配置接口,整理httpServerConfig配置,更合理配置,详细见setting/config相关
2.调整所有内置Cmds,stat用于显示server动态数据,setting用于显示server配置数据,logger用于显示日志信息
3.去除NioWorker类,不直接修改netty3源码的方式来获得内部信息,而是通过NioWorkerStat里面反射方式来获得信息
4.升级到netty3.2.1-Final


2010-07-14
1.增加LoggerCmd,可以实时修改logger level
2.增加IPGetter接口,从而实现通过反向代理时,可以获得真实IP的功能

2010-06-10
1.cmdmapper增加servelt2.2标准url-pattern匹配

2010-06-08
1.增加简单的url全路径匹配映射功能,CmdMappers
2.增加httpServer启动时自定义初始化Runner接口

2010-06-02
1.去除json-lib包,使用jackson以及server-util内置json编码器,提高格式化json性能
2.增加ipfilter的别名配置

2010-5-29
1.增加memcached的详细监控
2.增加对post的支持
3.ipfilter能够自动识别本机ip

2010-5-27
1.修改业务超时的处理方式： 去掉多余的 future.get()中断线程的方法,因其需要增加额外的线程池,现使用定时任务SimpleTimeoutThreadInterrupter来通知中断
2.修复编码统计的错误,一次响应会调用多次writeComplete

Q:加了线程池,反而效果不好,是指 大量并发的情况下。。最后会卡死
A:原因是线程池一定要使用netty自带的OrderedMemoryAwareThreadPoolExecutor,不然极有可能就会造成问题中所反应的“死锁”情况

Q:压力大的情况下,如果发包数量大时,编码确实会慢,这个时候,只用nioWorker时,搞不定
A:应该是业务处理慢,如果不使用线程池的话,会直接影响nioWorker的正常编码解码任务

2010-5-26
1.改正线程池(分拆pipeline的事件线程池跟具体业务超时线程池),解决大量请求后无法响应的问题
2.解决/stat/threads接口并发请求时直接让服务器崩溃的问题
3.解决/stat/nioworkers接口有时报空异常的问题

2010-5-25
1.增加ipAuthenticator,配置可以定时重载,调用方法可以针对不同业务指定不同配置

2010-5-24
1.增加ExecutionHandler,业务处理线程池使用OrderedMemoryAwareThreadPoolExecutor
2.暂时去除编码超时的处理
3.完善监控信息

2010-5-23
1.增加流量的监控
2.增加ip白名单
3.增加解包,发包的统计
